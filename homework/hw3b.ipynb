{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# HW3B - Pandas Fundamentals"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "See Canvas for details on how to complete and submit this assignment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "This assignment transitions you from NumPy's numerical array operations to Pandas' powerful tabular data manipulation. While NumPy excels at homogeneous numerical arrays, Pandas is designed for the heterogeneous, labeled data that characterizes most real-world datasets—mixing dates, categories, numbers, and text within the same table.\n",
    "\n",
    "You'll work with real bike share data from Chicago's Divvy system to answer questions about urban transportation patterns. Through three progressively complex problems—exploring usage patterns, analyzing rider behavior, and conducting temporal analysis—you'll discover why Pandas has become the standard tool for data analysis in Python.\n",
    "\n",
    "The assignment emphasizes Pandas' design philosophy: named column access, explicit indexing methods (loc/iloc), handling missing data, and method chaining for readable data pipelines. You'll also see how Pandas builds on NumPy while adding the structure and convenience needed for practical data science work.\n",
    "\n",
    "This assignment should take 3-5 hours to complete.\n",
    "\n",
    "Before submitting, ensure your notebook:\n",
    "\n",
    "- Runs completely with \"Kernel → Restart & Run All\"\n",
    "- Includes thoughtful responses to all interpretation questions\n",
    "- Uses clear variable names and follows good coding practices\n",
    "- Shows your work (don't just print final answers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "### Learning Objectives"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "By completing this assignment, you will be able to:\n",
    "\n",
    "1. **Construct and manipulate Pandas data structures**\n",
    "   - Create DataFrames from dictionaries and CSV files\n",
    "   - Distinguish between Series and DataFrame objects\n",
    "   - Set and reset index structures appropriately\n",
    "   - Understand when operations return views vs copies\n",
    "2. **Apply explicit indexing paradigms**\n",
    "   - Use `loc[]` for label-based data access\n",
    "   - Use `iloc[]` for position-based data access\n",
    "   - Access columns using bracket notation\n",
    "   - Explain when each indexing method is appropriate\n",
    "3. **Diagnose and explore datasets systematically**\n",
    "   - Use `info()`, `describe()`, `head()`, and `dtypes` to understand data structure\n",
    "   - Identify missing values with `isna()` and `notna()`\n",
    "   - Calculate summary statistics across different axes\n",
    "   - Interpret value distributions with `value_counts()`\n",
    "4. **Filter data with boolean indexing and queries**\n",
    "   - Combine multiple conditions with `&`, `|`, and `~` operators\n",
    "   - Use `isin()` for membership testing\n",
    "   - Apply `query()` for readable complex filters\n",
    "   - Understand how index alignment affects operations\n",
    "5. **Work with datetime data**\n",
    "   - Parse dates during CSV loading\n",
    "   - Extract temporal components with the `.dt` accessor\n",
    "   - Filter data by date ranges\n",
    "   - Create time-based derived features\n",
    "6. **Connect Pandas patterns to data analysis workflows**\n",
    "   - Formulate questions that data can answer\n",
    "   - Choose appropriate methods for different analysis tasks\n",
    "   - Interpret results in domain context\n",
    "   - Recognize when vectorized operations outperform apply()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "### Generative AI Allowance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "You may use GenAI tools for brainstorming, explanations, and code sketches if you disclose it, understand it, and validate it. Your submission must represent your own work and you are solely responsible for its correctness."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "### Scoring"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "Total of 90 points available, will be graded out of 80. Scores of >100% are allowed.\n",
    "\n",
    "Distribution:\n",
    "\n",
    "- Tasks: 48 pts\n",
    "- Interpretation: 32 pts\n",
    "- Reflection: 10 pts\n",
    "\n",
    "Points by Problem:\n",
    "\n",
    "- Problem 1: 3 tasks, 10 pts\n",
    "- Problem 2: 4 tasks, 14 pts\n",
    "- Problem 3: 4 tasks, 14 pts\n",
    "- Problem 4: 3 tasks, 10 pts\n",
    "\n",
    "Interpretation Questions:\n",
    "\n",
    "- Problem 1: 3 questions, 8 pts\n",
    "- Problem 2: 4 questions, 8 pts\n",
    "- Problem 3: 3 questions, 8 pts\n",
    "- Problem 4: 3 questions, 8 pts\n",
    "\n",
    "Graduate differentiation: poor follow-up responses will result in up to a 5pt deduction for that problem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "## Dataset: Chicago Divvy Bike Share"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "The dataset you will analyze is based on real trip information from Divvy, Chicago's bike share system. It contains individual trips with start/end times, station information, and rider type.\n",
    "\n",
    "Dataset homepage: https://divvybikes.com/system-data\n",
    "\n",
    "Each trip includes:\n",
    "\n",
    "- Trip start and end times (datetime)\n",
    "- Start and end station names and IDs\n",
    "- Rider type (member vs casual)\n",
    "- Bike type (classic, electric, or docked)\n",
    "\n",
    "Chicago's Department of Transportation uses this data to optimize station placement, understand usage patterns, and improve service. You'll explore similar questions that real transportation analysts investigate."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "## Problems"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "### Problem 1: Creating DataFrames from Scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "Before loading data from files, you need to understand how Pandas structures are built. In this problem, you'll create Series and DataFrames manually using Python's built-in data structures. This is a quick warmup to establish the fundamentals."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {},
   "source": [
    "#### Task 1a: Create a Series"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {},
   "source": [
    "Create a Series called `temperatures` representing daily high temperatures for a week:\n",
    "\n",
    "- Monday: 72°F\n",
    "- Tuesday: 75°F  \n",
    "- Wednesday: 68°F\n",
    "- Thursday: 71°F\n",
    "- Friday: 73°F\n",
    "\n",
    "Use the day names as the index. Print the Series and its data type."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17",
   "metadata": {},
   "source": [
    "##### Your Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Monday       72\n",
      "Tuesday      75\n",
      "Wednesday    68\n",
      "Thursday     71\n",
      "Friday       73\n",
      "dtype: int64\n",
      "\n",
      "\n",
      "int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Create the series\n",
    "temperatures : pd.core.series.Series = pd.Series([72, 75, 68, 71, 73], index=[\"Monday\", \"Tuesday\", \"Wednesday\", \"Thursday\", \"Friday\"])\n",
    "\n",
    "# Print the series and its data type\n",
    "print(temperatures)\n",
    "print(\"\\n\")\n",
    "print(temperatures.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19",
   "metadata": {},
   "source": [
    "#### Task 1b: Create a DataFrame from a Dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20",
   "metadata": {},
   "source": [
    "Create a DataFrame called `products` with the following data:\n",
    "\n",
    "| product | price | quantity |\n",
    "|---------|-------|----------|\n",
    "| Widget  | 19.99 | 100 |\n",
    "| Gadget  | 24.99 | 75 |\n",
    "| Doohickey | 12.49 | 150 |\n",
    "\n",
    "Use a dictionary where keys are column names and values are lists. Print the DataFrame and report its shape."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21",
   "metadata": {},
   "source": [
    "##### Your Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     product  price  quantity\n",
      "0     Widget  19.99       100\n",
      "1     Gadget  24.99        75\n",
      "2  Doohickey  12.49       150\n",
      "\n",
      "The DataFrame products has a size of 9\n"
     ]
    }
   ],
   "source": [
    "products_as_dict : dict = {\"product\" : [\"Widget\", \"Gadget\", \"Doohickey\"], \"price\" : [19.99, 24.99, 12.49], \"quantity\" : [100, 75, 150]}\n",
    "\n",
    "products : pd.core.frame.DataFrame = pd.DataFrame(products_as_dict)\n",
    "\n",
    "print(products)\n",
    "print(f\"\\nThe DataFrame products has a size of {products.size}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23",
   "metadata": {},
   "source": [
    "#### Task 1c: Access DataFrame Elements"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24",
   "metadata": {},
   "source": [
    "Using the `products` DataFrame from Task 1b, extract and print:\n",
    "\n",
    "1. The `price` column as a Series\n",
    "2. The `product` and `quantity` columns as a DataFrame (using a list of column names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25",
   "metadata": {},
   "source": [
    "##### Your Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    19.99\n",
      "1    24.99\n",
      "2    12.49\n",
      "Name: price, dtype: float64\n",
      "\n",
      "\n",
      "     product  quantity\n",
      "0     Widget       100\n",
      "1     Gadget        75\n",
      "2  Doohickey       150\n"
     ]
    }
   ],
   "source": [
    "# Print the price as a Series\n",
    "price_as_series : pd.core.series.Series = products[\"price\"]\n",
    "print(price_as_series)\n",
    "\n",
    "# Print the product and quantity as a DataFrame\n",
    "prod_quant_as_df : pd.core.frame.DataFrame = products[[\"product\", \"quantity\"]]\n",
    "print(\"\\n\")\n",
    "print(prod_quant_as_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27",
   "metadata": {},
   "source": [
    "#### Interpretation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28",
   "metadata": {},
   "source": [
    "Answer the following questions (briefly / concisely) in the markdown cell below:\n",
    "\n",
    "1. Data structure mapping: When you create a DataFrame from a dictionary (like in Task 1b), what do the dictionary keys become? What do the values become?\n",
    "2. Bracket notation: Why does `df['price']` return a Series, but `df[['price']]` return a DataFrame? What's the difference in what you're asking for?\n",
    "3. Index purpose: In Task 1a, you used day names as the index instead of default numbers (0, 1, 2...). When would a custom index like this be more useful than the default numeric index?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29",
   "metadata": {},
   "source": [
    "##### Your Answers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30",
   "metadata": {},
   "source": [
    "### Student Response:\n",
    "\n",
    "1. When a DataFrame is created from a dictionary, the keys become column headers, and the values become data points.\n",
    "2. `df\\['price'\\]` returns a Series because the index is one diminsional, while `df\\[\\['price'\\]\\]` returns a DataFrame because the index is two diminsional. When using double square brackets to index, you are asking pandas to return the dictionary with only those columns, while single square brackets asks to return a series of a given column.\n",
    "3. Using a custom index, as opposed to numerical indices, is more useful that the default index in instances where you may wish to plot data against this custom index, or an instance where the string values are unique (e.g. a users email)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31",
   "metadata": {},
   "source": [
    "### Problem 2: Loading and Initial Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32",
   "metadata": {},
   "source": [
    "Before starting this problem, make sure you are working in a copy of this file in the `my_repo` folder you created in HW2a. You must also have a copy of the file `202410-divvy-tripdata-100k.csv` in a subdirectory called `data`. That file structure is illustrated below.\n",
    "\n",
    "```text\n",
    "~/insy6500/my_repo\n",
    "└── homework\n",
    "    ├── data\n",
    "    │   └── 202410-divvy-tripdata-100k.csv\n",
    "    └── hw3b.ipynb\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33",
   "metadata": {},
   "source": [
    "#### Task 2a: Load and Understand Raw Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34",
   "metadata": {},
   "source": [
    "Start by loading the data \"as-is\" to get a general understanding of the overall structure and how Pandas interprets it by default.\n",
    "\n",
    "Note on file paths: The provided code uses `Path` from Python's `pathlib` module to handle file paths. Path objects work consistently across operating systems (Windows uses backslashes `\\`, Mac/Linux use forward slashes `/`), automatically using the correct separator for your system. The provided code defines `csv_path` which should be used as the filename in your `pd.read_csv` to load the data file.\n",
    "\n",
    "1. Use `pd.read_csv` to load `csv_path` (provided below) without specifying any other arguments. Assign it to the variable `df_raw`.\n",
    "2. Use the methods we described in class to explore the shape, structure, types, etc. of the data. In particular, consider which columns represent dates or categories.\n",
    "3. Note the amount of memory used by the dataset. See the section on memory diagnostics in notebook 07a for appropriate code snippets using `memory_usage`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35",
   "metadata": {},
   "source": [
    "##### Your Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df_raw has shape (100000, 13)\n",
      "\n",
      "\n",
      "Columns and their types:\n",
      "ride_id : object\n",
      "rideable_type : object\n",
      "started_at : object\n",
      "ended_at : object\n",
      "start_station_name : object\n",
      "start_station_id : object\n",
      "end_station_name : object\n",
      "end_station_id : object\n",
      "start_lat : float64\n",
      "start_lng : float64\n",
      "end_lat : float64\n",
      "end_lng : float64\n",
      "member_casual : object\n",
      "\n",
      "\n",
      "            ride_id  rideable_type               started_at  \\\n",
      "0  67BB74BD7667BAB7  electric_bike  2024-09-30 23:12:01.622   \n",
      "1  5AF1AC3BA86ED58C  electric_bike  2024-09-30 23:19:25.409   \n",
      "2  7961DD2FC1280CDC   classic_bike  2024-09-30 23:32:24.672   \n",
      "3  2E16892DEEF4CC19   classic_bike  2024-09-30 23:42:11.207   \n",
      "4  AAF0220F819BEE01  electric_bike  2024-09-30 23:49:25.380   \n",
      "\n",
      "                  ended_at         start_station_name start_station_id  \\\n",
      "0  2024-10-01 00:20:00.674     Oakley Ave & Touhy Ave           bdd4c3   \n",
      "1  2024-10-01 00:42:09.933                        NaN              NaN   \n",
      "2  2024-10-01 00:23:18.647     St. Clair St & Erie St           9c619a   \n",
      "3  2024-10-01 00:10:16.831  Ashland Ave & Chicago Ave           72a04d   \n",
      "4  2024-10-01 00:06:27.476          900 W Harrison St           11da85   \n",
      "\n",
      "           end_station_name end_station_id  start_lat  start_lng    end_lat  \\\n",
      "0                       NaN            NaN  42.012342 -87.688243  41.970000   \n",
      "1    Benson Ave & Church St         a10cf0  42.070000 -87.730000  42.048214   \n",
      "2  LaSalle St & Illinois St         fbd1ad  41.894345 -87.622798  41.890762   \n",
      "3    Loomis St & Archer Ave         896337  41.895954 -87.667728  41.841633   \n",
      "4         900 W Harrison St         11da85  41.874754 -87.649807  41.874754   \n",
      "\n",
      "     end_lng member_casual  \n",
      "0 -87.650000        casual  \n",
      "1 -87.683485        casual  \n",
      "2 -87.631697        member  \n",
      "3 -87.657435        casual  \n",
      "4 -87.649807        member  \n",
      "\n",
      "63.70 MB\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "# create a OS-independent pointer to the csv file created by Setup\n",
    "csv_path = Path('./data/202410-divvy-tripdata-100k.csv')\n",
    "\n",
    "# load and explore the data below (create additional code / markdown cells as necessary)\n",
    "\n",
    "# Load the raw data\n",
    "df_raw : pd.core.frame.DataFrame = pd.read_csv(csv_path)\n",
    "\n",
    "# Explore the data\n",
    "print(f\"df_raw has shape {df_raw.shape}\")\n",
    "\n",
    "print(\"\\n\")\n",
    "\n",
    "#Print each column name, and its data type\n",
    "print(\"Columns and their types:\")\n",
    "column_dtype_pairs : str = (\"\\n\").join([(f\"{col} : {df_raw[col].dtype}\") for col in df_raw.columns])\n",
    "print(column_dtype_pairs)\n",
    "\n",
    "# See the head of the DataFrame\n",
    "print(\"\\n\")\n",
    "print(df_raw.head())\n",
    "\n",
    "print(f\"\\n{df_raw.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37",
   "metadata": {},
   "source": [
    "#### Task 2b: Reload with Proper Data Types"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38",
   "metadata": {},
   "source": [
    "1. Repeat step 2a.1 to reload the data. Use the `dtype` and `parse_dates` arguments to properly assign categorical and date types. Assign the result to the variable name `rides`.\n",
    "2. After loading, use `rides.info()` to confirm the type changes.\n",
    "3. Use `memory_usage` to compare the resulting size with that from step 2a.3."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39",
   "metadata": {},
   "source": [
    "##### Your Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 100000 entries, 0 to 99999\n",
      "Data columns (total 13 columns):\n",
      " #   Column              Non-Null Count   Dtype         \n",
      "---  ------              --------------   -----         \n",
      " 0   ride_id             100000 non-null  object        \n",
      " 1   rideable_type       100000 non-null  category      \n",
      " 2   started_at          100000 non-null  datetime64[ns]\n",
      " 3   ended_at            100000 non-null  datetime64[ns]\n",
      " 4   start_station_name  89623 non-null   category      \n",
      " 5   start_station_id    89623 non-null   category      \n",
      " 6   end_station_name    89485 non-null   category      \n",
      " 7   end_station_id      89485 non-null   category      \n",
      " 8   start_lat           100000 non-null  float64       \n",
      " 9   start_lng           100000 non-null  float64       \n",
      " 10  end_lat             99913 non-null   float64       \n",
      " 11  end_lng             99913 non-null   float64       \n",
      " 12  member_casual       100000 non-null  category      \n",
      "dtypes: category(6), datetime64[ns](2), float64(4), object(1)\n",
      "memory usage: 6.4+ MB\n",
      "None\n",
      "\n",
      "Categories Memory Usage:\n",
      "12.85 MB\n",
      "\n",
      "Raw Memory Usage:\n",
      "63.70 MB\n",
      "The categories instance is much smaller\n"
     ]
    }
   ],
   "source": [
    "# Reload the data\n",
    "rides : pd.core.frame.DataFrame = pd.read_csv(csv_path, dtype=\n",
    "                                              {\"ride_id\" : str,\n",
    "                                              \"rideable_type\" : \"category\",\n",
    "                                              \"start_station_name\" : \"category\",\n",
    "                                              \"start_station_id\" : \"category\",\n",
    "                                              \"end_station_name\" : \"category\",\n",
    "                                              \"end_station_id\" : \"category\",\n",
    "                                              \"start_lat\" : float,\n",
    "                                              \"start_long\" : float,\n",
    "                                              \"end_lat\" : float,\n",
    "                                              \"end_long\" : float,\n",
    "                                              \"member_casual\" : \"category\"}, \n",
    "                                              parse_dates= [\"started_at\", \"ended_at\"])\n",
    "\n",
    "print(rides.info())\n",
    "\n",
    "print(\"\\nCategories Memory Usage:\")\n",
    "print(f\"{rides.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
    "print(\"\\nRaw Memory Usage:\")\n",
    "print(f\"{df_raw.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
    "print(\"The categories instance is much smaller\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41",
   "metadata": {},
   "source": [
    "#### Task 2c: Explore Structure and Missing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42",
   "metadata": {},
   "source": [
    "Using the `rides` DataFrame from Task 2b:\n",
    "\n",
    "1. Determine the range of starting dates in the dataframe using the `min` and `max` methods.\n",
    "2. Count the number of missing values in each column. See the section of the same name in lecture 06b.\n",
    "3. Convert the Series from step 2 to a DataFrame using `.to_frame(name='count')`, then add a column called 'percentage' that calculates the percentage of missing values for each column."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43",
   "metadata": {},
   "source": [
    "##### Your Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The starting dates range from 2024-09-30 23:12:01.622000 to 2024-10-31 23:54:02.851000\n",
      "\n",
      "                    count  percentage\n",
      "ride_id                 0       0.000\n",
      "rideable_type           0       0.000\n",
      "started_at              0       0.000\n",
      "ended_at                0       0.000\n",
      "start_station_name  10377      10.377\n",
      "start_station_id    10377      10.377\n",
      "end_station_name    10515      10.515\n",
      "end_station_id      10515      10.515\n",
      "start_lat               0       0.000\n",
      "start_lng               0       0.000\n",
      "end_lat                87       0.087\n",
      "end_lng                87       0.087\n",
      "member_casual           0       0.000\n"
     ]
    }
   ],
   "source": [
    "# Determine the range of the starting dates\n",
    "start_date_min = rides[\"started_at\"].min()\n",
    "start_date_max = rides[\"started_at\"].max()\n",
    "print(f\"The starting dates range from {start_date_min} to {start_date_max}\\n\")\n",
    "\n",
    "# Count the number of missing values in each column, then convert this Series into a DataFrame\n",
    "missing_val_counts : pd.core.series.Series = rides.isna().sum()\n",
    "missing_val_counts_df : pd.core.frame.DataFrame = missing_val_counts.to_frame(name='count')\n",
    "\n",
    "# Add a column for percent missing\n",
    "missing_val_counts_df[\"percentage\"] = [((missing_val_counts_df.loc[idx, \"count\"]/len(rides[idx]))*100) for idx in missing_val_counts_df.index]\n",
    "\n",
    "# Print the resulting DataFrame\n",
    "print(missing_val_counts_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45",
   "metadata": {},
   "source": [
    "#### Task 2d: Create Trip Duration Column and Set Index\n",
    "\n",
    "Before setting the index, create a derived column for trip duration:\n",
    "\n",
    "1. Calculate trip_duration_min by subtracting `started_at` from `ended_at`, then converting to minutes using `.dt.total_seconds() / 60`\n",
    "3. Display basic statistics (min, max, mean) for the new column using `.describe()`\n",
    "4. Show the first few rows with `started_at`, `ended_at`, and `trip_duration_min` to verify the calculation\n",
    "5. Set `started_at` as the DataFrame's index. Verify the change by printing the index and displaying the first few rows."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46",
   "metadata": {},
   "source": [
    "##### Your Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count    100000.000000\n",
      "mean         16.144576\n",
      "std          52.922539\n",
      "min           0.006533\n",
      "25%           5.489271\n",
      "50%           9.423592\n",
      "75%          16.407171\n",
      "max        1499.949717\n",
      "Name: trip_duration_min, dtype: float64\n",
      "\n",
      "\n",
      "               started_at                ended_at  trip_duration_min\n",
      "0 2024-09-30 23:12:01.622 2024-10-01 00:20:00.674          67.984200\n",
      "1 2024-09-30 23:19:25.409 2024-10-01 00:42:09.933          82.742067\n",
      "2 2024-09-30 23:32:24.672 2024-10-01 00:23:18.647          50.899583\n",
      "\n",
      "\n",
      "\n",
      "The index first is: 2024-09-30 23:12:01.622000\n",
      "\n",
      "\n",
      "\n",
      "                                  ride_id  rideable_type  \\\n",
      "started_at                                                 \n",
      "2024-09-30 23:12:01.622  67BB74BD7667BAB7  electric_bike   \n",
      "2024-09-30 23:19:25.409  5AF1AC3BA86ED58C  electric_bike   \n",
      "2024-09-30 23:32:24.672  7961DD2FC1280CDC   classic_bike   \n",
      "\n",
      "                                       ended_at      start_station_name  \\\n",
      "started_at                                                                \n",
      "2024-09-30 23:12:01.622 2024-10-01 00:20:00.674  Oakley Ave & Touhy Ave   \n",
      "2024-09-30 23:19:25.409 2024-10-01 00:42:09.933                     NaN   \n",
      "2024-09-30 23:32:24.672 2024-10-01 00:23:18.647  St. Clair St & Erie St   \n",
      "\n",
      "                        start_station_id          end_station_name  \\\n",
      "started_at                                                           \n",
      "2024-09-30 23:12:01.622           bdd4c3                       NaN   \n",
      "2024-09-30 23:19:25.409              NaN    Benson Ave & Church St   \n",
      "2024-09-30 23:32:24.672           9c619a  LaSalle St & Illinois St   \n",
      "\n",
      "                        end_station_id  start_lat  start_lng    end_lat  \\\n",
      "started_at                                                                \n",
      "2024-09-30 23:12:01.622            NaN  42.012342 -87.688243  41.970000   \n",
      "2024-09-30 23:19:25.409         a10cf0  42.070000 -87.730000  42.048214   \n",
      "2024-09-30 23:32:24.672         fbd1ad  41.894345 -87.622798  41.890762   \n",
      "\n",
      "                           end_lng member_casual  trip_duration_min  \n",
      "started_at                                                           \n",
      "2024-09-30 23:12:01.622 -87.650000        casual          67.984200  \n",
      "2024-09-30 23:19:25.409 -87.683485        casual          82.742067  \n",
      "2024-09-30 23:32:24.672 -87.631697        member          50.899583  \n"
     ]
    }
   ],
   "source": [
    "# Get the trip duration in minutes\n",
    "rides[\"trip_duration_min\"] = ((rides[\"ended_at\"] - rides[\"started_at\"]).dt.total_seconds()/60)\n",
    "\n",
    "# Use .describe() to display basic statistics of this Series\n",
    "print(rides[\"trip_duration_min\"].describe())\n",
    "\n",
    "# Show the first few rows with the specified columns\n",
    "print(\"\\n\")\n",
    "print(rides[[\"started_at\", \"ended_at\", \"trip_duration_min\"]].head(3))\n",
    "\n",
    "# Set the index as \"started_at\" and print the first few rows\n",
    "rides = rides.set_index(\"started_at\")\n",
    "print(f\"\\n\\n\\nThe index first is: {rides.index[0]}\\n\\n\\n\")\n",
    "print(rides.head(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48",
   "metadata": {},
   "source": [
    "#### Interpretation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49",
   "metadata": {},
   "source": [
    "Reflect on problem 2 and answer (briefly / concisely) the following questions:\n",
    "\n",
    "1. What types did Pandas assign to `started_at` and `member_casual` in Task 2a? Why might these defaults be problematic?\n",
    "2. Look at the values in the station ID fields. Based on what you learned about git commit IDs in HW3a, how do you think the station IDs were derived?\n",
    "3. Explain in your own words what method chaining is, what `df.isna().sum()` does and how it works.\n",
    "4. Assume you found ~10% missing values in station columns but ~0% in coordinates. What might explain this? How might you handle the affected rows?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50",
   "metadata": {},
   "source": [
    "##### Your Answers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51",
   "metadata": {},
   "source": [
    "### Student Response:\n",
    "1. Pandas assigned `started_at` and `member_casual` as `object` data types. This can be problomatic because it can be difficult to do operations on these ambigous data types. The datetime type and category type allowed easier operations, and memory efficiencies, respectively.\n",
    "2. Station IDs were possibly obtained by computing the cryptographic hash of the station name, to guarantee a uniqe station ID per station.\n",
    "3. Method chaining is the act of calling methods on objects returned by other methods. `df.isna().sum()` first computes a boolean mask of `df`, and then takes the sum of the resulting columns in the DataFrame, collapsing the DataFrame into a Series. This chain is a way to compute the number of missing data values per column.\n",
    "4. This situation could be explained by riders leaving their rideables _near_ but not within the geofence boundary of the station. To handle affected rows, you could assign stations to rides if the coordinates are within some tolerance distance of the station coordinates."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52",
   "metadata": {},
   "source": [
    "#### Follow-Up (Graduate Students Only)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53",
   "metadata": {},
   "source": [
    "Compare memory usage results in 2a.3 and 2b.3. What caused the change? Why are these numbers different from what is reported at the bottom of `df.info()`? Which should you use if data size is a concern?\n",
    "\n",
    "Working with DataFrames typically requires 5-10x the dataset size in available RAM. On a system with 16GB, assuming about 30% overhead from the operating system and other programs, what range of dataset sizes would be safely manageable? Calculate using both 5x (optimistic) and 10x (conservative) multipliers, then explain which you'd recommend for reliable work."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54",
   "metadata": {},
   "source": [
    "##### Your Answers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55",
   "metadata": {},
   "source": [
    "### Student Response\n",
    "2a.3 used 64MB of memory, while 2b.3 used approximately 13MB of memory. The change is driven by the use of categories. Categories store each value only once and reuse duplicates, saving memory for repeat values. The value reported from the use of `df.info()` only includes the memory impact of pointers to values, and not the values themselves. If data size is a concern, use the value that includes the values themselves, and not just the pointers. \n",
    "\n",
    "Assuming 30% overhead for various programs and OS PIDs, there are 11.2 GB of memory remaining for use.\n",
    "\n",
    "**For the optimistic case:**\n",
    "We can solve for maxmium data size by solving 5x = 11.2, for x. Here, x = 2.24 GB\n",
    "\n",
    "**For the pessimistic case:**\n",
    "We can solve for maxmium data size by solving 10x = 11.2, for x. Here, x = 1.12 GB\n",
    "\n",
    "I would reccomend working with datasets no larger than 1.12 GB on a machine with 16 GB of RAM. This allows plenty of overhead in the event that another OS process or program starts using more RAM."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56",
   "metadata": {},
   "source": [
    "### Problem 3: Filtering and Transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57",
   "metadata": {},
   "source": [
    "With clean data loaded, you can now filter and transform it to answer specific questions. This problem focuses on Pandas' powerful indexing and filtering capabilities, along with creating derived columns that enable deeper analysis.\n",
    "\n",
    "You'll continue working with the `rides` DataFrame from Problem 2, which has `started_at` set as the index."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "58",
   "metadata": {},
   "source": [
    "#### Task 3a: Boolean Indexing and Membership Testing\n",
    "\n",
    "Use boolean indexing and the `isin()` method to answer these questions:\n",
    "\n",
    "1. How many trips were taken by *members* using *electric bikes*? Use `&` to combine conditions.\n",
    "2. What percentage of all trips does this represent?\n",
    "3. How many trips started at any of these three stations: \"Streeter Dr & Grand Ave\", \"DuSable Lake Shore Dr & Monroe St\", or \"Kingsbury St & Kinzie St\"? Use `isin()`.\n",
    "\n",
    "Note: Remember to use parentheses around each condition when combining with `&`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59",
   "metadata": {},
   "source": [
    "##### Your Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There were 33121 trips taken by members on electric bikes.\n",
      "This represents 33.12% of all trips.\n",
      "2702 trips began in one of the specified stations.\n"
     ]
    }
   ],
   "source": [
    "# Compute the number of trips taken by members usiung electric bikes\n",
    "trips_with_members : pd.core.frame.DataFrame = rides[\"member_casual\"] == \"member\"\n",
    "trips_with_ebikes : pd.core.frame.DataFrame = rides[\"rideable_type\"] == \"electric_bike\"\n",
    "\n",
    "count_rides_members_ebikes : int = len(rides[(trips_with_ebikes) & (trips_with_members)])\n",
    "print(f\"There were {count_rides_members_ebikes} trips taken by members on electric bikes.\")\n",
    "print(f\"This represents {count_rides_members_ebikes/len(rides):.2%} of all trips.\")\n",
    "\n",
    "# Compute the number of trips that started in one of the specified stations\n",
    "tgt_stations : list = [\"Streeter Dr & Grand Ave\" ,\"DuSable Lake Shore Dr & Monroe St\", \"Kingsbury St & Kinzie St\"]\n",
    "count_trip_started_in_specified : int = len(rides[rides[\"start_station_name\"].isin(tgt_stations)])\n",
    "print(f\"{count_trip_started_in_specified} trips began in one of the specified stations.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61",
   "metadata": {},
   "source": [
    "#### Task 3b: Create Derived Columns from Datetime\n",
    "\n",
    "Add two categorical columns to the rides DataFrame based on trip start time:\n",
    "\n",
    "1. `is_weekend`: Boolean column that is True for Saturday/Sunday trips. Use .dt.dayofweek on the index (Monday=0, Sunday=6).\n",
    "2. `time_of_day`: String categories based on start hour:\n",
    "   - \"Morning Rush\" if hour is 7, 8, or 9\n",
    "   - \"Evening Rush\" if hour is 16, 17, or 18\n",
    "   - \"Midday\" for all other hours\n",
    "\n",
    "For step 2, initialize the column to \"Midday\", then use .loc[mask, 'time_of_day'] with boolean masks to assign rush hour categories. Extract hour using .dt.hour on the index.\n",
    "\n",
    "After creating both columns, use value_counts() on time_of_day to show the distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62",
   "metadata": {},
   "source": [
    "##### Your Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distribution of time_of_day\n",
      "time_of_day\n",
      "Midday          55912\n",
      "Evening Rush    28218\n",
      "Morning Rush    15870\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Create a column called is_weekend to have a boolean value of true if the start time of the trip is on a weekend\n",
    "weekend_bool_mask : pd.core.frame.DataFrame = rides.index.dayofweek.isin([5,6])\n",
    "rides[\"is_weekend\"] = weekend_bool_mask\n",
    "rides[\"is_weekend\"] = rides[\"is_weekend\"].astype(\"category\")\n",
    "\n",
    "# Create a column called time_of_day that categorizes each ride based on the time of day\n",
    "\n",
    "# Initialize on Midday\n",
    "rides[\"time_of_day\"] = \"Midday\"\n",
    "\n",
    "# Mask the Morning Rush case\n",
    "rides.loc[rides.index.hour.isin([7, 8, 9]), \"time_of_day\"] = \"Morning Rush\"\n",
    "\n",
    "# Mask the Evening Rush case\n",
    "rides.loc[rides.index.hour.isin([16, 17, 18]), \"time_of_day\"] = \"Evening Rush\"\n",
    "\n",
    "# Convert the column to a category column\n",
    "rides[\"time_of_day\"] = rides[\"time_of_day\"].astype(\"category\")\n",
    "\n",
    "print(\"Distribution of time_of_day\")\n",
    "print(rides[\"time_of_day\"].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64",
   "metadata": {},
   "source": [
    "#### Task 3c: Complex Filtering with query()\n",
    "\n",
    "Use the `query()` method to find trips that meet **all** of these criteria:\n",
    "- Casual riders (not members)\n",
    "- Weekend trips  \n",
    "- Duration greater than 20 minutes\n",
    "- Electric bikes\n",
    "\n",
    "Report:\n",
    "1. How many trips match these criteria?\n",
    "2. What percentage of all trips do they represent?\n",
    "3. What is the average duration of these trips?\n",
    "\n",
    "Hint: Column names work directly in `query()` strings. Combine conditions with `and`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65",
   "metadata": {},
   "source": [
    "##### Your Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "REPORT:\n",
      "Trips matching criteria: 1501\n",
      "Percent of total trips matching criteria: 1.50%\n",
      "Average duration of trips meeting criteria: 40.37 minutes\n"
     ]
    }
   ],
   "source": [
    "# Use the query() method to find the trips that meet all of the provided criteria\n",
    "rides_meeting_criteria : pd.core.frame.DataFrame = rides.query(\"member_casual == 'casual' and is_weekend == True and trip_duration_min > 20 and rideable_type == 'electric_bike'\", engine='python')\n",
    "\n",
    "# Of the resulting trips, find the average of the duration\n",
    "avg_duration_meet_criteria : float = rides_meeting_criteria[\"trip_duration_min\"].mean()\n",
    "\n",
    "print(\"REPORT:\")\n",
    "print(f\"Trips matching criteria: {len(rides_meeting_criteria)}\")\n",
    "print(f\"Percent of total trips matching criteria: {len(rides_meeting_criteria)/len(rides):.2%}\")\n",
    "print(f\"Average duration of trips meeting criteria: {avg_duration_meet_criteria:.2f} minutes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67",
   "metadata": {},
   "source": [
    "#### Task 3d: Explicit Indexing Practice\n",
    "\n",
    "Practice using `loc[]` and `iloc[]` for different selection tasks:\n",
    "\n",
    "1. Use `iloc[]` to select the first 10 trips, showing only `member_casual`, `rideable_type`, and `trip_duration_min` columns\n",
    "2. Use `loc[]` to select trips from October 15-17 (use date strings `'2024-10-15':'2024-10-17'`), showing the same three columns\n",
    "3. Count how many trips occurred during this date range\n",
    "\n",
    "Note: When using `iloc[]`, remember it's position-based (0-indexed). When using `loc[]` with the datetime index, you can slice using date strings."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68",
   "metadata": {},
   "source": [
    "##### Your Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7235 trips occurred from Oct. 15 to Oct. 17th.\n"
     ]
    }
   ],
   "source": [
    "# Select first 10 trips with member_casual, rideable_type, and trip_duration_min columns only\n",
    "first_10_3cols : pd.core.frame.DataFrame = rides[[\"member_casual\", \"rideable_type\", \"trip_duration_min\"]].iloc[:10]\n",
    "\n",
    "# Select trips from Oct. 15-17, with the same three cols\n",
    "october_trips : pd.core.frame.DataFrame = rides.loc['2024-10-15':'2024-10-17', [\"member_casual\", \"rideable_type\", \"trip_duration_min\"]]\n",
    "\n",
    "# Report the number of trips that occured in this date range\n",
    "print(f\"{len(october_trips)} trips occurred from Oct. 15 to Oct. 17th.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70",
   "metadata": {},
   "source": [
    "#### Interpretation\n",
    "\n",
    "Reflect on this problem and answer (briefly / concisely) the following questions:\n",
    "\n",
    "1. `isin()` advantages: Compare using `isin(['A', 'B', 'C'])` versus `(col == 'A') | (col == 'B') | (col == 'C')`. Beyond readability, what practical advantage does `isin()` provide when filtering for many values (e.g., 20+ stations)?\n",
    "2. Conditional assignment order: In Task 3b, why did we initialize all values to \"Midday\" before assigning rush hour categories? What would go wrong if you assigned categories in a different order, or didn't set a default?\n",
    "3. `query()` vs boolean indexing: The `query()` method in Task 3c could have been written with boolean indexing instead. When would you choose `query()` over boolean indexing? When might boolean indexing be preferable despite being more verbose?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71",
   "metadata": {},
   "source": [
    "##### Your Answers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72",
   "metadata": {},
   "source": [
    "### Student Response\n",
    "1. Using `isin()` is not only more readable, but allows the user to build a dynamic list of values to test against (using list comprehension on some values with perhaps even a set of logical conditions. This method also allows the user to write code that is not contained in a single, very long line.\n",
    "2. If we did not set a default, or initialize all values as Midday, we would have to have a very long list of elements to test each values memebership against. There also is not a clean mechanic for \"else\" logic in boolean assignement.\n",
    "3. The `query()` method is better for several logical tests in conjunction. However, boolean indexing might be preferable, despite being more verbose, in instances where compound conditions, or more complex compound conditions are tested for (e.g. (A and B) or C if C in someList). Boolean indexing might also be prefered when variables represent the same name as column names, or a dynamic condition is needed.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73",
   "metadata": {},
   "source": [
    "#### Follow-Up (Graduate Students Only)\n",
    "\n",
    "Pandas supports a variety of indexing paradigms, including bracket notation (`df['col']`), label-based indexing (`loc[]`), and position-based indexing (`iloc[]`). The lecture recommended using bracket notation only for columns, and loc/iloc for everything else. Explain the rationale: why is this approach better than using bracket notation for everything, even though `df[0:5]` technically works for row slicing?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74",
   "metadata": {},
   "source": [
    "##### Your Answers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75",
   "metadata": {},
   "source": [
    "### Student Response\n",
    "\n",
    "Bracket notation, while technically viable, is best reserved for column selection because it is more readable. Being consistent with use of bracket notation produces more readable code. For example, `df[0:5]` appears to be an index label slice, however, if the index was a date, as in this assignment, the fact that this expression conducts positon indexing produces ambigous results. Using `loc[]` for label based indexing solves this problem by explicitly conveying to the reader that label based indexing is being used. Similarly `iloc[]` usage conveys explicitly that position based indexing is being used. The differentiator is that by sticking to the prescribed convention above, intent is always clear and consistent."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76",
   "metadata": {},
   "source": [
    "### Problem 4: Temporal Analysis and Export"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77",
   "metadata": {},
   "source": [
    "Time-based patterns are crucial for understanding bike share usage. In this problem, you'll analyze when trips occur, how usage differs between rider types, and export filtered results. You'll use the datetime index you set in Problem 2 and the derived columns from Problems 2-3."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78",
   "metadata": {},
   "source": [
    "#### Task 4a: Identify Temporal Patterns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79",
   "metadata": {},
   "source": [
    "Use the datetime index to extract temporal components and identify usage patterns:\n",
    "\n",
    "1. Extract the *hour* from the index and use `value_counts()` to find the most popular hour for trips. Report the peak hour and how many trips occurred during that hour.\n",
    "2. Extract the *day name* from the index and use `value_counts()` to find the busiest day of the week. Report the day and number of trips.\n",
    "3. Sort the results from step 2 to show days in order from Monday to Sunday (not by trip count). Use `sort_index()`.\n",
    "\n",
    "Hint: Use `.dt.hour` and `.dt.day_name()` on the datetime index."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80",
   "metadata": {},
   "source": [
    "##### Your Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The peak hour is hour 17 with 10574 rides.\n",
      "The peak day is day Wednesday with 16513 rides.\n",
      "\n",
      "Sorted Results:\n",
      "started_at\n",
      "Monday       11531\n",
      "Tuesday      14970\n",
      "Wednesday    16513\n",
      "Thursday     16080\n",
      "Friday       13691\n",
      "Saturday     14427\n",
      "Sunday       12788\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Get a Series of the value_counts() and find the max\n",
    "index_hours_counts : pd.core.frame.DataFrame = (rides.index.hour).value_counts()\n",
    "print(f\"The peak hour is hour {index_hours_counts.idxmax()} with {index_hours_counts.max()} rides.\")\n",
    "\n",
    "# Extract the day name from the index and find the busiest day\n",
    "index_days_counts : pd.core.frame.DataFrame = rides.index.day_name().value_counts()\n",
    "print(f\"The peak day is day {index_days_counts.idxmax()} with {index_days_counts.max()} rides.\")\n",
    "\n",
    "# Sort the index days counts by day name\n",
    "day_order = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\n",
    "print(\"\\nSorted Results:\")\n",
    "print(index_days_counts.sort_index(key=lambda day : pd.Categorical(day, categories=day_order)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82",
   "metadata": {},
   "source": [
    "#### Task 4b: Compare Groups with groupby()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83",
   "metadata": {},
   "source": [
    "Use `groupby()` (introduced in 07a) to compare trip characteristics across different groups:\n",
    "\n",
    "1. Calculate the average trip duration by rider type (`member_casual`). Which group takes longer trips on average?\n",
    "2. Calculate the average trip duration by bike type (`rideable_type`). Which bike type has the longest average trip?\n",
    "3. Count the number of trips by rider type using `groupby()` with `.size()`. Compare this with using `value_counts()` on the `member_casual` column - do they give the same result?\n",
    "\n",
    "Note: Use single-key groupby only (one column at a time)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84",
   "metadata": {},
   "source": [
    "##### Your Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The casual rider takes the longest trips on average with an average trip duration of 23.98 minutes.\n",
      "The classic_bike rideable takes the longest trips on average with an average trip duration of 20.34 minutes.\n",
      "\n",
      "Trips by rider type using the .groupby().size() method:\n",
      "member_casual\n",
      "casual    34686\n",
      "member    65314\n",
      "Name: trip_duration_min, dtype: int64\n",
      "\n",
      "Trips by rider type using the value_counts() method:\n",
      "member_casual\n",
      "member    65314\n",
      "casual    34686\n",
      "Name: count, dtype: int64\n",
      "34686\n"
     ]
    }
   ],
   "source": [
    "# Calculate average trip duration by rider type. Find the group that takes the longer trips on average.\n",
    "rider_type_avg_duration : pd.core.series.Series = rides.groupby('member_casual', observed=False)['trip_duration_min'].mean()\n",
    "print(f\"The {rider_type_avg_duration.idxmax()} rider takes the longest trips on average with an average trip duration of {rider_type_avg_duration.max():.2f} minutes.\")\n",
    "\n",
    "# Calculate the average trip duration by bike type.\n",
    "bike_type_avg_duration : pd.core.series.Series = rides.groupby('rideable_type', observed=False)['trip_duration_min'].mean()\n",
    "print(f\"The {bike_type_avg_duration.idxmax()} rideable takes the longest trips on average with an average trip duration of {bike_type_avg_duration.max():.2f} minutes.\")\n",
    "\n",
    "# Count the number of trips by rider type - groupby().size() method\n",
    "trips_by_rider_type_method1 : pd.core.series.Series = rides.groupby('member_casual', observed=False)['trip_duration_min'].size()\n",
    "print(\"\\nTrips by rider type using the .groupby().size() method:\")\n",
    "print(trips_by_rider_type_method1)\n",
    "\n",
    "# Count the number of trips by rider type - value_counts() method\n",
    "trips_by_rider_type_method2 : pd.core.series.Series = rides['member_casual'].value_counts()\n",
    "print(\"\\nTrips by rider type using the value_counts() method:\")\n",
    "print(trips_by_rider_type_method2)\n",
    "\n",
    "# These methods produce the same result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86",
   "metadata": {},
   "source": [
    "#### Task 4c: Filter, Sample, and Export"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87",
   "metadata": {},
   "source": [
    "Create a filtered dataset for weekend electric bike trips and export it:\n",
    "\n",
    "The provided code once again uses Path to create an `output` directory and constructs the full file path as `output/weekend_electric_trips.csv`. Use the `output_file` variable when calling `.to_csv()`.\n",
    "\n",
    "1. Filter for trips where `is_weekend == True` and `rideable_type == 'electric_bike'`\n",
    "2. Use `iloc[]` to select the first 1000 trips from this filtered dataset\n",
    "3. Use `reset_index()` to convert the datetime index back to a column (so it's included in the export)\n",
    "4. Export to CSV with filename `weekend_electric_trips.csv`, including only these columns: `started_at`, `ended_at`, `member_casual`, `trip_duration_min`, `time_of_day`\n",
    "5. Use `index=False` to avoid writing the default numeric index to the file\n",
    "\n",
    "After exporting, report how many total weekend electric bike trips existed before sampling to 1000."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88",
   "metadata": {},
   "source": [
    "##### Your Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before filtering, there were 13026 weekend ebike trips.\n"
     ]
    }
   ],
   "source": [
    "# do not modify this setup code\n",
    "from pathlib import Path\n",
    "\n",
    "output_dir = Path('output')\n",
    "output_dir.mkdir(exist_ok=True)\n",
    "output_file = output_dir / 'weekend_electric_trips.csv'\n",
    "\n",
    "# Filter for trips taken during the weekend and the rideable is the electric bike\n",
    "weekend_ebike_trips : pd.core.frame.DataFrame = rides.query(\"is_weekend == True and rideable_type == 'electric_bike'\", engine=\"python\")\n",
    "\n",
    "# Update the DataFrame to only include the first 1000 results\n",
    "weekend_ebike_trips_sample : pd.core.frame.DataFrame = weekend_ebike_trips.iloc[:1000]\n",
    "\n",
    "# Reset the index to include the datetime index back into a column\n",
    "weekend_ebike_trips_sample.reset_index(inplace=True)\n",
    "\n",
    "# Export specified columns to csv\n",
    "weekend_ebike_trips_sample.to_csv(output_file, index=False, columns=[\"started_at\", \"ended_at\", \"member_casual\", \"trip_duration_min\", \"time_of_day\"])\n",
    "\n",
    "# Report the number of weekend ebike trips\n",
    "print(f\"Before filtering, there were {len(weekend_ebike_trips)} weekend ebike trips.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90",
   "metadata": {},
   "source": [
    "#### Interpretation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91",
   "metadata": {},
   "source": [
    "Reflect on this problem and answer the following questions:\n",
    "\n",
    "1. `groupby() conceptual model`: Explain in your own words what `groupby()` does. Use the phrase \"split-apply-combine\" in your explanation and describe what happens at each stage.\n",
    "2. `value_counts()` vs `groupby()`: In Task 4b.3, you compared two approaches for counting trips by rider type. When would you use `value_counts()` versus `groupby().size()`? Is there a situation where only one of them would work?\n",
    "3. Index management for export: In Task 4c, why did we use `reset_index()` before exporting? What would happen if you exported with the datetime index still in place and used `index=False`?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92",
   "metadata": {},
   "source": [
    "##### Your Answers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93",
   "metadata": {},
   "source": [
    "### Student Response\n",
    "1. `groupby()` is a Pandas operation that applies the \"split-apply-combine\" operation to a DataFrame, or Series object. First, Pandas splits data by values in the specified column. Then, a function is applied to each independant group (sum(), size(), mean(), etc.). Finally, the function results for each group are combined into a single data structure (DataFrame or Series).\n",
    "2. The `value_counts()` method is more suited for determining the frequency of many unique values in a single column, while the `groupby().size()` method is more well suited for determining the frequency of multiple columns' various combinations.\n",
    "3. Had we not reset the index before exporting, with index=False, the started_at column would have been lost, and not reported in the csv export."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94",
   "metadata": {},
   "source": [
    "#### Follow-Up (Graduate Students Only)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95",
   "metadata": {},
   "source": [
    "Compare `CSV` and _pickle_ formats for data storage and retrieval.\n",
    "\n",
    "Pickle is Python's built-in serialization format that saves Python objects exactly as they exist in memory, preserving all data types, structures, and metadata. Unlike CSV (which converts everything to text), pickle is binary (not human readable) and maintains the complete state of your DataFrame. Also, pickle files only work in Python, while CSV is universal. Read more in the [Pandas documentation](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.to_pickle.html).\n",
    "\n",
    "The code below investigates an interesting pattern: Do riders take longer trips from scenic lakefront stations even during rush hours? This could indicate tourists or recreational riders using these popular locations for leisure trips during typical commute times. The analysis filters for trips over 15 minutes that started from lakefront stations during morning (7-9am) or evening (4-6pm) rush hours, sorted by duration to see the longest trips first.\n",
    "\n",
    "Run the code below, then answer the interpretation questions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 310 long rush-hour trips from lakefront stations\n",
      "\n",
      "CSV file size: 40.57 KB\n",
      "Pickle file size: 55.16 KB\n",
      "Size difference: 14.59 KB\n",
      "\n",
      "Load time comparison:\n",
      "CSV:\n",
      "986 μs ± 1.52 μs per loop (mean ± std. dev. of 7 runs, 1,000 loops each)\n",
      "\n",
      "Pickle:\n",
      "410 μs ± 1.51 μs per loop (mean ± std. dev. of 7 runs, 1,000 loops each)\n",
      "\n",
      "Data types from CSV (without parse_dates):\n",
      "started_at             object\n",
      "ended_at               object\n",
      "start_station_name     object\n",
      "end_station_name       object\n",
      "member_casual          object\n",
      "rideable_type          object\n",
      "trip_duration_min     float64\n",
      "dtype: object\n",
      "\n",
      "Data types from Pickle:\n",
      "started_at            datetime64[ns]\n",
      "ended_at              datetime64[ns]\n",
      "start_station_name          category\n",
      "end_station_name            category\n",
      "member_casual               category\n",
      "rideable_type               category\n",
      "trip_duration_min            float64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# the following lines were commented out since they were run in 4c\n",
    "# from pathlib import Path\n",
    "# output_dir = Path('output')\n",
    "\n",
    "csv_file = output_dir / 'lakefront_rush_trips.csv'\n",
    "pickle_file = output_dir / 'lakefront_rush_trips.pkl'\n",
    "\n",
    "# Filter for interesting pattern: Long trips (>15 min) during rush hours \n",
    "# from lakefront stations, sorted by duration\n",
    "lakefront_rush = (rides\n",
    "    .loc[(rides.index.hour.isin([7, 8, 9, 16, 17, 18]))]\n",
    "    .loc[(rides['start_station_name'].str.contains('Lake Shore|Lakefront', \n",
    "                                                    case=False, \n",
    "                                                    na=False))]\n",
    "    .loc[rides['trip_duration_min'] > 15]\n",
    "    .sort_values('trip_duration_min', ascending=False)\n",
    "    .head(1000)\n",
    "    .reset_index()\n",
    "    [['started_at', 'ended_at', 'start_station_name', 'end_station_name',\n",
    "      'member_casual', 'rideable_type', 'trip_duration_min']]\n",
    ")\n",
    "\n",
    "print(f\"Found {len(lakefront_rush)} long rush-hour trips from lakefront stations\")\n",
    "\n",
    "# Export to both formats\n",
    "lakefront_rush.to_csv(csv_file, index=False)\n",
    "lakefront_rush.to_pickle(pickle_file)\n",
    "\n",
    "# Compare file sizes\n",
    "csv_size = os.path.getsize(csv_file) / 1024  # Convert to KB\n",
    "pickle_size = os.path.getsize(pickle_file) / 1024\n",
    "print(f\"\\nCSV file size: {csv_size:.2f} KB\")\n",
    "print(f\"Pickle file size: {pickle_size:.2f} KB\")\n",
    "print(f\"Size difference: {abs(csv_size - pickle_size):.2f} KB\")\n",
    "\n",
    "# Compare load times\n",
    "print(\"\\nLoad time comparison:\")\n",
    "print(\"CSV:\")\n",
    "%timeit pd.read_csv(csv_file)\n",
    "print(\"\\nPickle:\")\n",
    "%timeit pd.read_pickle(pickle_file)\n",
    "\n",
    "# Check data type preservation\n",
    "# Note: CSV load without parse_dates loses datetime types\n",
    "csv_loaded = pd.read_csv(csv_file)\n",
    "pickle_loaded = pd.read_pickle(pickle_file)\n",
    "\n",
    "print(\"\\nData types from CSV (without parse_dates):\")\n",
    "print(csv_loaded.dtypes)\n",
    "print(\"\\nData types from Pickle:\")\n",
    "print(pickle_loaded.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97",
   "metadata": {},
   "source": [
    "After running the code, answer these questions:\n",
    "\n",
    "1. Method chaining: The analysis uses method chaining with a specific formatting pattern:\n",
    "\n",
    "   ```python\n",
    "   result = (df\n",
    "       .method1()\n",
    "       .method2()\n",
    "       .method3()\n",
    "   )\n",
    "   ```\n",
    "\n",
    "   This wraps the entire chain in parentheses, allowing each method to appear on its own line without backslashes. Discuss why this makes formatting more readable, how it makes debugging easier, how it relates to seeing changes in the code with git diff, and what downsides heavy chaining might have.\n",
    "3. Data types: Compare the dtypes from CSV versus pickle. What types were preserved by pickle that were lost in CSV? Why is this preservation significant for subsequent analysis?\n",
    "4. Trade-offs: Given your observations about size, speed, and type preservation, when would you choose pickle over CSV for your work? When would CSV still be the better choice despite pickle's advantages?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98",
   "metadata": {},
   "source": [
    "### Student Response\n",
    "1. Method chaining makles code more readable because the reader can examine the overall option in a step-by-step manner. Furthermore, this makes debugging easier by being able to look at each method operation individually. This relates to seeing changes in git diff in the sense that git diff will show the change to each line (each method call) as opposed to changes in a single line only (a single .method1().method2() call. Heavy chaining can cause issues if a more central method call has an error, it can be difficult to detect, due to output transformation though other method calls.\n",
    "2. Data types in the CSV are not maintained from the DataFrame, while the pickle retains the data type from the DataFrame. The category and datetime data types are the types that are retained by the pickle. This is significant for subsequent analysis because the datatime retention cuts down on duplicate processing to cast those values back into datetime vales. The retention of the category type is critical for subsequent analsysis because it reduces memory use by reusing reoccuring values.\n",
    "3. The pickle file is slightly larger than the csv file, but loads in under half of the time. Pickle is best used when speed matters, when memory usage matters, or subsequent analysis with special types will occur. The CSV file is the better choice when a human readable file is required, or you need to share the file with someone who may use a different language to perform subsequent analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99",
   "metadata": {},
   "source": [
    "## Reflection\n",
    "\n",
    "Address the following questions in a markdown cell:\n",
    "\n",
    "1. NumPy vs Pandas\n",
    "   - What was the biggest conceptual shift moving from NumPy arrays to Pandas DataFrames?\n",
    "   - Which Pandas concept was most challenging: indexing (loc/iloc), missing data, datetime operations, or method chaining? How did you work through it?\n",
    "2. Real Data Experience\n",
    "   - How did working with real CSV data (with missing values, datetime strings, etc.) differ from hw2b's synthetic NumPy arrays?\n",
    "   - Based on this assignment, what makes Pandas well-suited for data analysis compared to pure NumPy?\n",
    "3. Learning & Application\n",
    "   - Which new skill from this assignment will be most useful for your own data work?\n",
    "   - On a scale of 1-10, how prepared do you feel to use Pandas for your own projects? What would increase that score?\n",
    "4. Feedback\n",
    "   - Time spent: ___ hours (breakdown optional)\n",
    "   - Most helpful part of the assignment: ___\n",
    "   - One specific improvement suggestion: ___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "100",
   "metadata": {},
   "source": [
    "### Your Answers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "101",
   "metadata": {},
   "source": [
    "### Student Response\n",
    "1. The biggest conceptual shift when moving from NumPy arrays to Pandas DataFrames is indexing. While NumPy offers a plethora of methods to index data, Pandas indexing relies more on column headers, and potentially complex index values to index the data structures. The most difficult Pandas concept is probably datetime operations. I struggled with identifying when to use the dt class, and when not to. Pandas documentation helps, and using an LLM to help decipher when to use which classmethod to call on which objects allowed me to work through this issue.\n",
    "2. Working with real CSV data differs from working with NumPy arrays because the CSV data is heterogenous, which means a decent amount of setup is required to ensure data types are all being loaded in the right way, and the most efficient way (e.g. using category data types). Pandas is well-suited for data analysis when comapred to NumpPy since NumPy arrays are homogenous. Data is rarely homogenous across a data set, so there are limitations to what can be done with pure NumPy.\n",
    "3. I think the `.query()` method is most useful to my own data work. I often analyze data at work where 5+ conditions must be tested for, each one affecting the rest. Using this query method will allow me to conduct these checks in a single line. I feel that my Pandas familiarity is about a 7/10. I believe that this will increase with the completion of the class project. Furthermore, when time allows, I plan to examine the Pandas class code for DataFrame and Series to understand which methods can be performed on which data types, and how they are to be used for each type.\n",
    "4. I spent about 5 cumulative hours on this assignment. Sometimes, I worked on this assignment late at night after work, so I think I could have done it in about 3 hours if I worked on it during the day, for what it is worth. The most helpful parts of these assignments to me are the coding portions. I would like to see these assignments have more analysis and fewer reflection questions. I find the content derived from the reflection questions to be quite helpful, though. Perhaps some of the simpler conclusions from these questions can be conveyed in the problem statments (i.e. Use this method, now use this method, notice that method 1 is more readable/more concise/etc. than method 2) similar to what was done in the first homework."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (insy6500)",
   "language": "python",
   "name": "insy6500"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
